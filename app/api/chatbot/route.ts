import { type NextRequest, NextResponse } from "next/server";
import { streamText, type CoreMessage } from "ai";
import { groq } from "@ai-sdk/groq";
import { findSimilarChunksEnhanced, hybridSearch, type EnhancedChunk } from "@/lib/db";
import { HfInference, type FeatureExtractionOutput } from "@huggingface/inference";

export const maxDuration = 300;

const hf = new HfInference(process.env.HUGGINGFACE_API_KEY);

// Cache para embeddings de consultas similares (opcional)
const embeddingCache = new Map<string, number[]>();

// Funci√≥n para determinar el tipo de consulta y estrategia RAG apropiada
function analyzeQueryType(query: string): {
  isSpecific: boolean;
  requiresContext: boolean;
  searchStrategy: 'hybrid' | 'similarity' | 'enhanced';
  chunkCount: number;
} {
  const query_lower = query.toLowerCase();
  
  // Detectar consultas espec√≠ficas que necesitan m√°s contexto
  const specificIndicators = [
    'qu√© dice sobre', 'seg√∫n el documento', 'en el texto', 'menciona',
    'define', 'explica detalladamente', 'cu√°l es la diferencia',
    'paso a paso', 'procedimiento', 'proceso'
  ];
  
  const isSpecific = specificIndicators.some(indicator => 
    query_lower.includes(indicator)
  );
  
  // Detectar si la consulta requiere m√∫ltiples fuentes o contexto amplio
  const contextIndicators = [
    'compara', 'diferencias', 'similitudes', 'relaci√≥n entre',
    'en general', 'resumen', 'overview', 'panorama'
  ];
  
  const requiresContext = contextIndicators.some(indicator => 
    query_lower.includes(indicator)
  );

  // Determinar estrategia de b√∫squeda
  let searchStrategy: 'hybrid' | 'similarity' | 'enhanced' = 'enhanced';
  let chunkCount = 8;

  if (query.length > 100 || query.split(' ').length > 15) {
    // Consulta compleja - usar b√∫squeda h√≠brida
    searchStrategy = 'hybrid';
    chunkCount = 10;
  } else if (isSpecific) {
    // Consulta espec√≠fica - similarity search enfocada
    searchStrategy = 'similarity';
    chunkCount = 6;
  } else if (requiresContext) {
    // Consulta que requiere contexto amplio
    searchStrategy = 'enhanced';
    chunkCount = 12;
  }

  return { isSpecific, requiresContext, searchStrategy, chunkCount };
}

// Funci√≥n para formatear contexto con metadatos enriquecidos
function formatEnhancedContext(chunks: EnhancedChunk[]): {
  contextText: string;
  sourceSummary: string;
  hasHighRelevance: boolean;
} {
  if (chunks.length === 0) {
    return {
      contextText: "No se encontr√≥ informaci√≥n relevante en los documentos.",
      sourceSummary: "Sin fuentes disponibles",
      hasHighRelevance: false
    };
  }

  // Agrupar chunks por secci√≥n para mejor organizaci√≥n
  const chunksBySection = chunks.reduce((acc, chunk) => {
    const section = chunk.section_title || 'Contenido General';
    if (!acc[section]) acc[section] = [];
    acc[section].push(chunk);
    return acc;
  }, {} as Record<string, EnhancedChunk[]>);

  let contextText = "CONTEXTO EXTRA√çDO DE LOS DOCUMENTOS:\n\n";
  let sourceCount = 0;

  // Formatear contexto por secciones
  Object.entries(chunksBySection).forEach(([section, sectionChunks]) => {
    contextText += `üìö **${section}**\n`;
    
    sectionChunks.forEach((chunk, index) => {
      sourceCount++;
      const relevanceIcon = chunk.similarity_score > 0.7 ? "üéØ" : 
                           chunk.similarity_score > 0.5 ? "üìç" : "üìÑ";
      
      contextText += `\n[FUENTE ${sourceCount}] ${relevanceIcon} `;
      if (chunk.created_with_overlap) {
        contextText += `(Contexto expandido) `;
      }
      contextText += `\n${chunk.chunk_text}\n`;
    });
    contextText += "\n";
  });

  const avgRelevance = chunks.reduce((sum, c) => sum + c.similarity_score, 0) / chunks.length;
  const hasHighRelevance = avgRelevance > 0.5;

  const sourceSummary = `${sourceCount} fuentes encontradas`;

  return { contextText, sourceSummary, hasHighRelevance };
}

// Funci√≥n para crear prompt optimizado seg√∫n el tipo de consulta
function createOptimizedPrompt(
  contextText: string,
  queryAnalysis: ReturnType<typeof analyzeQueryType>,
  sourceSummary: string,
  hasHighRelevance: boolean
): CoreMessage {
  
  const basePersona = `Eres TUTOR-IA, un asistente educativo experto especializado en esta asignatura.`;
  
  let instructions = "";
  let responseFormat = "";

  if (queryAnalysis.isSpecific) {
    instructions = `Tu tarea es proporcionar una respuesta precisa y detallada bas√°ndote EXCLUSIVAMENTE en el contexto proporcionado. Cita espec√≠ficamente las fuentes usando [FUENTE X].`;
    responseFormat = `Estructura tu respuesta as√≠:
1. **Respuesta directa** - Responde la pregunta espec√≠fica
2. **Detalles adicionales** - Proporciona contexto relevante si est√° disponible
3. **Fuentes** - Menciona qu√© fuentes utilizaste`;
  } else if (queryAnalysis.requiresContext) {
    instructions = `Tu tarea es sintetizar informaci√≥n de m√∫ltiples fuentes para dar una visi√≥n comprehensiva del tema.`;
    responseFormat = `Estructura tu respuesta as√≠:
1. **Resumen ejecutivo** - Idea principal en 2-3 l√≠neas
2. **Puntos clave** - Los aspectos m√°s importantes con sus fuentes
3. **Informaci√≥n adicional** - Detalles complementarios si est√°n disponibles`;
  } else {
    instructions = `Tu tarea es responder de manera clara y educativa, usando el contexto disponible.`;
    responseFormat = `Proporciona una respuesta bien estructurada con vi√±etas o p√°rrafos seg√∫n sea apropiado.`;
  }

  const confidenceNote = hasHighRelevance 
    ? "El contexto proporcionado tiene alta relevancia para tu consulta."
    : "Ten en cuenta que el contexto disponible puede tener relevancia limitada para tu consulta espec√≠fica.";

  const fallbackBehavior = `

**IMPORTANTE - Reglas de respuesta:**
- Si la informaci√≥n no est√° en el contexto: "No encuentro informaci√≥n espec√≠fica sobre esto en el material de la asignatura disponible."
- Si el contexto es insuficiente: "La informaci√≥n disponible es limitada. Te recomiendo consultar material adicional o contactar al instructor."  
- NUNCA inventes o asumas informaci√≥n que no est√© expl√≠citamente en el contexto.
- Siempre cita las fuentes usando el formato [FUENTE X] cuando uses informaci√≥n espec√≠fica.

${confidenceNote}`;

  const fullPrompt = `${basePersona}

${instructions}

${responseFormat}

CONTEXTO DISPONIBLE (${sourceSummary}):
${contextText}

${fallbackBehavior}`;

  return {
    role: "system",
    content: fullPrompt
  };
}

export async function POST(request: NextRequest) {
  try {
    const { messages, subjectId }: { messages: CoreMessage[]; subjectId: string } = await request.json();

    // Validaciones mejoradas
    if (!subjectId) {
      throw new Error("El ID de la asignatura (subjectId) es requerido.");
    }

    if (!messages || messages.length === 0) {
      throw new Error("No se proporcionaron mensajes.");
    }

    const userQuery = messages[messages.length - 1]?.content;
    if (typeof userQuery !== "string" || userQuery.trim().length < 3) {
      throw new Error("La pregunta del usuario no es v√°lida o es demasiado corta.");
    }

    console.log(`üîé Consulta del usuario: "${userQuery}" para la asignatura ${subjectId}`);

    // Analizar tipo de consulta para optimizar la estrategia RAG
    const queryAnalysis = analyzeQueryType(userQuery);
    console.log(`üß† An√°lisis de consulta:`, queryAnalysis);

    // Generar embedding (con cache opcional)
    const cacheKey = userQuery.toLowerCase().trim();
    let embedding: number[];

    if (embeddingCache.has(cacheKey)) {
      embedding = embeddingCache.get(cacheKey)!;
      console.log("‚úÖ Embedding recuperado del cache");
    } else {
      console.log("üîÑ Generando nuevo embedding...");
      const embeddingResponse: FeatureExtractionOutput = await hf.featureExtraction({
        model: "sentence-transformers/multi-qa-mpnet-base-dot-v1",
        inputs: userQuery,
      });

      // Manejo robusto de la respuesta de embedding
      if (Array.isArray(embeddingResponse) && typeof embeddingResponse[0] === 'number') {
        embedding = embeddingResponse as number[];
      } else if (Array.isArray(embeddingResponse) && Array.isArray(embeddingResponse[0])) {
        embedding = embeddingResponse[0] as number[];
      } else {
        console.error("Estructura de embedding inesperada:", embeddingResponse);
        throw new Error("La respuesta de la API de embeddings no tuvo un formato de vector v√°lido.");
      }

      // Guardar en cache (limitar tama√±o del cache)
      if (embeddingCache.size < 100) {
        embeddingCache.set(cacheKey, embedding);
      }
      console.log("‚úÖ Embedding generado y almacenado en cache");
    }

    // Ejecutar b√∫squeda RAG optimizada seg√∫n el an√°lisis
    let contextChunks: EnhancedChunk[] = [];

    try {
      switch (queryAnalysis.searchStrategy) {
        case 'hybrid':
          console.log("üîç Ejecutando b√∫squeda h√≠brida (similarity + keywords)");
          contextChunks = await hybridSearch(
            subjectId, 
            userQuery, 
            embedding, 
            {
              match_count: queryAnalysis.chunkCount,
              keyword_weight: 0.3,
              similarity_threshold: 0.25
            }
          );
          break;

        case 'similarity':
          console.log("üéØ Ejecutando b√∫squeda por similarity enfocada");
          contextChunks = await findSimilarChunksEnhanced(
            subjectId, 
            embedding, 
            {
              match_count: queryAnalysis.chunkCount,
              similarity_threshold: 0.4,
              diversify_results: false,
              include_overlap_chunks: true
            }
          );
          break;

        case 'enhanced':
        default:
          console.log("üîß Ejecutando b√∫squeda enhanced con diversificaci√≥n");
          contextChunks = await findSimilarChunksEnhanced(
            subjectId, 
            embedding, 
            {
              match_count: queryAnalysis.chunkCount,
              similarity_threshold: 0.3,
              diversify_results: true,
              include_overlap_chunks: true
            }
          );
          break;
      }
    } catch (ragError) {
      console.error("‚ùå Error en b√∫squeda RAG, fallback a b√∫squeda b√°sica:", ragError);
      // Fallback a funci√≥n b√°sica si las nuevas fallan
      const { findSimilarChunks } = await import("@/lib/db");
      const basicChunks = await findSimilarChunks(subjectId, embedding, 5);
      contextChunks = basicChunks.map(chunk => ({
        chunk_text: chunk.chunk_text,
        section_title: 'Contenido',
        chunk_index: 0,
        similarity_score: 0.5,
        token_count: chunk.chunk_text.split(' ').length,
        created_with_overlap: false
      }));
    }

    console.log(`üìö Encontrados ${contextChunks.length} chunks de contexto`);
    if (contextChunks.length > 0) {
      const avgRelevance = contextChunks.reduce((sum, c) => sum + c.similarity_score, 0) / contextChunks.length;
      console.log(`üìä Relevancia promedio: ${(avgRelevance * 100).toFixed(1)}%`);
    }

    // Formatear contexto con metadatos enriquecidos
    const { contextText, sourceSummary, hasHighRelevance } = formatEnhancedContext(contextChunks);

    // Crear prompt optimizado
    const augmentedSystemPrompt = createOptimizedPrompt(
      contextText, 
      queryAnalysis, 
      sourceSummary, 
      hasHighRelevance
    );

    // Configuraci√≥n optimizada para Groq
    const modelConfig = {
      model: groq(process.env.MODEL || "llama-3.1-8b-instant"),
      messages: [augmentedSystemPrompt, ...messages],
      temperature: queryAnalysis.isSpecific ? 0.1 : 0.3, // Menos creatividad para respuestas espec√≠ficas
      max_tokens: queryAnalysis.requiresContext ? 1000 : 600, // M√°s tokens para respuestas comprehensivas
      top_p: 0.9,
      frequency_penalty: 0.1,
      presence_penalty: 0.1
    };

    console.log(`üöÄ Iniciando stream de Groq con modelo optimizado`);
    console.log(`‚öôÔ∏è Configuraci√≥n: temp=${modelConfig.temperature}, max_tokens=${modelConfig.max_tokens}`);

    const result = await streamText(modelConfig);

    return result.toDataStreamResponse();
    
  } catch (error) {
    console.error("‚ùå Error en el chatbot RAG mejorado:", error);
    
    // Error handling m√°s detallado
    let errorMessage = "Ocurri√≥ un error procesando tu consulta.";
    let statusCode = 500;

    if (error instanceof Error) {
      if (error.message.includes("API")) {
        errorMessage = "Error temporal del servicio. Por favor, intenta nuevamente.";
        statusCode = 503;
      } else if (error.message.includes("embedding")) {
        errorMessage = "Error procesando tu consulta. Verifica que tu pregunta sea clara.";
        statusCode = 400;
      } else if (error.message.includes("subjectId")) {
        errorMessage = "ID de asignatura inv√°lido.";  
        statusCode = 400;
      } else {
        errorMessage = error.message;
      }
    }

    return NextResponse.json({ 
      error: errorMessage,
      details: process.env.NODE_ENV === 'development' ? error : undefined
    }, { status: statusCode });
  }
}